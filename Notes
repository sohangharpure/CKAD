https://medium.com/@harioverhere/ckad-certified-kubernetes-application-developer-my-journey-3afb0901014

------------
Architecture
------------

Node - either a physical machine or a VM
Cluster - set of nodes

Master - Slave architecture
Master watches over worker nodes

Components
----------
API-server: Front-end for kubernetes, interaction point. Determines which one is master.
etcd: key-value store, distributed and reliable - stores all information regarding nodes/masters
kubelet: Agent on each nodes. Makes sure containers are running on each node. Interacts with master
Container runtime: Underlying software for running containers(dockers)
Controller: Brain behind the orchestration. Make scaling decisions
Scheduler: Distributing work

Master Node         Worker node(minion)
-----------         -------------------
kube-api-server     Container runtime
etcd                Kubelet
Controller
Scheduler


------------
POD's
------------

Smallest unit of deployment: POD
Can possibly have multiple containers per pod (helper containers). The other pod cannot be of the same type
    - Two containers can connect with each other using localhost
To scale up - create new pods. not add multiple containers within the same pod.

Scaling up and linking with docker ->
docker run helper -link app1 |  For this, we need to setup networking, shareable volumes etc. which can be automatically
docker run helper -link app2 |  managed by kubernetes

------
YAML's
------

4 mandatory fields in each kubernetes configuration file:

apiVersion - v1 for pod and service, apps/v1 for ReplicaSet, Deployment etc.
kind  - Type of object we are trying to create(Pod, Service etc.)
metadata - Data about the object. Dictionary. We can only specify kubernetes specific properties (name, labels etc)
           If you do want to specify custom properties, we can specify them under labels
spec - Provide info regarding the object we are trying to create(pod, deployment)
       Is a Dictionary
       

------------------------------------------------
Kubernetes Controllers
------------------------------------------------
Replication Controller -> High Availability, Share load across multiple pods

Like to have more than 1 instance(pod) of our application.
Even if you have a single pod, if existing one fails, it will bring up new one
Load balancing

Replication Controller      | ReplicaSet
-------------------------------------------------
Older tech                  | New recommended way
apiVersion: v1              | apiVersion: apps/v1   ->If wrong, you will get an error saying "no match for kind ReplicaSet"
kind: ReplicationController | kind: ReplicaSet
spec:                       | spec:
    - template:             |   - template:      
         {POD template}     |       {POD template}
    - replicas: 2           |   - replicas: 2
                            |   - selector:                 -> This is because ReplicaSet's can also manage pods which are not created by itself
                            |        matchLabels:              This field is mandatory and cannot be skipped
                            |           type: frontEnd      

Labels and Selectors:
ReplicaSet: Monitor all pods, if failed, bring up new ones
How does it know what pods to monitor ? Labeling our pods!

----------
Deployment
----------
The definition is exactly the same as a ReplicaSet except with the deployment keyword
Also there are different types such as rolling deployment, or completely stopping existing instances and then bringing up new ones etc.
While deploying, it creates a new ReplicaSet and then after the upgrade it deletes existing deployments

----------
Namespaces
----------
Used for isolation within an environment
Default namespace - automatically created by Kubernetes when cluster startup.
kube-system - all objects internal to kubernets are stored here so that user might not touch them accidently.
kube-public - resources which need to be made public to all users
Can assign quotas and limits etc to each namespace. As well as who can access what(policies)

Can connect to another service within another namespace via it's fully qualified name. e.g:

db-service.dev.svc.cluster.local 
where:
cluster.local - domain name 
svc - subdomain for the service
dev - cluster name
db-service - name of the service

--------------
Resource Quota
--------------
To limit resources in a namespace, create a resource quota
kind = ResourceQuota
metadata:
    name: compute-quota
    namespace: dev
spec: 
    hard:
        pods: "10"


-----------------------------
Commands, arguments in docker
-----------------------------
docker run ubuntu -> Runs and exits
Containers only lives as long as process inside is alive

docker run ubuntu [COMMAND] 

FROM ubuntu     -> To make the command change permanant
CMD sleep 5

CMD ["COMMAND", "param1"] -> 1st param to json should be the command to run

Build custom image -> docker build -t ubuntu-sleeper .
Run custom image -> docker run ubuntu-sleeper

Make sleep duration custom ->
docker run ubuntu-sleeper 10

FROM ubuntu
ENTRYPOINT ["sleep"] -> This will get appended

Difference between sleep and ENTRYPOINT:
Sleep -> Whatever parameters you specify will replace the existing one
ENTRYPOINT -> Whatever parameters you specify will get appended

what if we want a default value if argument is not specified?
ENTRYPOINT ["sleep"]
CMD ["5"]

docker run ubuntu-sleeper 10 -> sleep 10
docker run ubuntu-sleeper -> sleep 5

What if we want to specify a custom ENTRYPOINT:
docker run --entrypoint sleep2.0 ubuntu-sleeper 10
Command at startup: sleep2.0 10

---------------------------------
Commands, arguments in Kubernetes
---------------------------------
apiVersion: v1
kind: Pod
metadata:
    name: ubuntu-sleeper-pod
spec:
    containers:
        - name: ubuntu-sleeper
          image: ubuntu-sleeper
          command: ["sleep2.0"] -> Equivalent to overriding ENTRYPOINT for docker image
          args: ["10"] -> Equivalent to overriding CMD (arguments) to docker image


-----------------------------------
environment variables in Kubernetes
-----------------------------------
apiVersion: v1
kind: Pod
metadata:
    name: ubuntu-sleeper-pod
spec:
    containers:
        - name: ubuntu-sleeper
          image: ubuntu-sleeper
          env:
            - name: APP_COLOR -> Name of the environment variable
              value: pink     -> Value of the environment variable

----------
Config Map
----------
When there are a lot of environment variables and pod definitions, it becomes difficult to manage
them. For this, we use config maps
When you inject a variable from config map into a pod, they become available as environment variables

There are 2 phases to config map:
1. Define config map
2. Inject them into pod

Imperative way -> kubectl create configmap
Declarative -> kubectl create -f ..

-------
Secrets
-------
Store sensitive information
Similar to config maps, but hash encoded

There are 2 phases to Secrets:
1. Define secrets
2. Inject them into pod

Imperative way -> kubectl create secret generic
Declarative -> kubectl create -f ..

Secrets are not encrypted, so it is not safer in that sense. However, some best practices around using secrets make it safer. As in best practices like:

Not checking-in secret object definition files to source code repositories.
Enabling Encryption at Rest for Secrets so they are stored encrypted in ETCD.

Also the way kubernetes handles secrets. Such as:

A secret is only sent to a node if a pod on that node requires it.
Kubelet stores the secret into a tmpfs so that the secret is not written to disk storage.
Once the Pod that depends on the secret is deleted, kubelet will delete its local copy of the secret data as well.

Having said that, there are other better ways of handling sensitive data like passwords in Kubernetes, such as using tools like Helm Secrets, HashiCorp Vault

----------------
Security Context
----------------

------------------------------------------------------------------------------------------------------------------------------

Security in docker:
Containers are not fully isolated from hosts, they are bound to the host kernel
Container and host each have a namespace
All processes in containers are actually run on the host but in their own namespace
Docker host can only see processes in it's own Namespaces
When you list processes in docker host, you only see processes running within the conatiner
However, when you run list processes on the host, you will see all processes but with a different process id

By default docker runs processes as root user
You can set the --user command in docker run if you don't want to run it as root
Another way to enforce is in the docker image

FROM ubuntu
USER 1000

root user within the container is different from root on the host
root user capabilities -> /usr/include/linux/capability.h

By default, docker provides limited functionaoltiy to the root user.

To add more capabilities, you can use the below command:
docker run --cap-add MAC_ADMIN ubuntu
To remove capabilities
docker run --cap-drop MAC_ADMIN ubuntu
To run with all capabilities
docker run --privileges 

------------------------------------------------------------------------------------------------------------------------------

Kubernetes security:

Can be controlled at a container level or a pod level
If you define security at pod level, it will override security for all containers
If you define at both pod and container level, container level will override pod level

apiVersion: v1
kind: Pod
metadata:
    name: web-pod
spec:
    securityContext:        -> Pod Level
        runAsUser: 1000
    containers:
        - name: ubuntu
          image: ubuntu
          securityContext:        -> Container Level
            runAsUser: 1000
            capabilities:           -> capabilities can only be added at container level, not POD level
              add: ["MAC_ADMIN"]


---------------------
Resource Requirements
---------------------

Kubernetes Scheduler determines which pod shold go on which node
If the scheduler finds no node has enought resources to host the pod, it will avoid running the pod and the status will be "Pending"

Default resources per pod (resource request) -> When scheduling, looks at these
.5 CPU, 256 Mi of memory,

In the yaml, under the container, resources can be specified as below:
resources:
    requests:
        memory: "1Gi"
        cpu: 1

What is 1 count of CPU? 
0.1 CPU -> 100m 
1 CPU is Equivalent to 1 VCPU = 1 AWS vCPU, 1 GCP Core, 1 Azure Core, 1 Hyperthread

memory:
1 GB is 1000 Megabyte
1 Gi(Gibibyte) = 1024 Mibibyte

In Docker, there is no default limit.

By default, kubernetes defaults 1 vCPU limit, 512 Mi memory

To change limits:
    requests:
        memory: "1Gi"
        cpu: 1
    limits:
        memory: "2Gi"
        cpu: 2

When resources go beyond limit, kubernets throttles CPU. For memory, it it's exceeding the limit for a long time, kubernetes terminates the pod


----------------
Service Accounts
----------------

Two types of accounts in kubernetes: 
User -> Used by users/developers
service -> Used by applications to interact with kubernetes
    For e.g Jenkins, Prometheus

When a service account is created, it by default generates a token which can be used to authenticate
It's stored as a secret
Secret object is linked to a service account

What if the third party application(such as Jenkins) is hosted in the kubernetes cluster itself?
Whole process of getting the token and configuring it can be done automatically by mounting a volume with the secret
Don't have to provide it manually

"default" service account -> Each namespace has it's own. 
Whenver a pod is created, the default service account is mounted as a volume in the pod.

Default service account is restricted

To specify custom service account in a container:
containers:
    - name: xx
      image: xxx
    serviceAccount: dashboard-sa

If you don't want to mount the default Service account:
    automountServiceAccountToken: false

----------------------
Taints and Tolerations
----------------------
Nothing to do with security - used to set restrictions on what pods can be scheduled on the nodes by the Scheduler
For e.g, I want that all pods for a certain application should be placed on Node 1.
2 Steps:
a)  Apply a taint to the Node1 with a value. For e.g - Taint=blue
    By default, there are no tolerations placed on a pod. Hence no pods can be placed on Node1 with that toleration.
b) Apply toleration on a pod with value blue
Taints are applied on a node, Tolerations are applied on the pod

taint-effect -> specifies what happens to pod that DO NOT TOLERATE this taint?
3 values:
a) NoSchedule : Pods will not be scheduled on the node
b) PreferNoSchedule: System will try to avoid placing the pod on the node but no guaratee
c) NoExecute: No new pods will be scheduled, existing pods will be evicted from the node if they do not tolerate the taint

Applying toleration on a pod:
spec:
    containers:
        xxxx
    tolerations:
    - key: "app"                 -> same as taint
      operator: "Equal"
      value: "blue"
      effect:  "NoSchedule"

Taints and tolerations tells the node to accept a pod with certain toleration, not the other way round(meaning that this pod should go
to this particular node)
If you want to specify certain pod should go to certain node, it's achieved via "Node Affinity"

Scheduler never schedules a pod on the master node. Why?
When a kubernetes cluster is setup, a default taint is set on the master node
kubectl describe node kubemaster | grep Taint

--------------
Node Selectors
--------------
Configure which pods go to which nodes. Node Affinity also does the same
spec:
    containers:
        - name: xxx
          image: xxx
        nodeSelector:
            size: Large  -> These are labels assigned to the node. Scheduler uses this
                            Label should exist before assigning pod to node.

Limitation -> cannot have multiple conditions such as AND, OR etc.

-------------
Node Affinity
-------------
Configure which pods go to which nodes
Provides advanced capabilities

spec:
    containers:
        - name: xxx
          image: xxx
    affinity:
        nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                    - matchExpressions:
                        - key: size
                          operator: In
                          values:
                            - Large
                            - Medium

operator -> In, NotIn, Exists - Simply checks if something exists. No "values" required
What if someone changes the node labels, or there is no label matching the node selector? 
    What happens to the existing pods on the node?

Scheduling -> Before creating the pod and assigning it
Executing -> Pod is already running and changes are made to the pod such as updating labels etc.

requiredDuringSchedulingIgnoredDuringExecution -> 
If a matching node does not exist, pod will never be scheduled
preferredDuringSchedulingIgnoredDuringExecution -> 
If no matching nodes are found, ignore affinity rules and scheduler will assign it to a random node

Planned:
requiredDuringSchedulingRequiredDuringExecution

Two states of a pod:
                                                    DuringScheduling      DuringExecution
requiredDuringSchedulingIgnoredDuringExecution          Required             Ignored              -> This means that the running pod will not be affected
preferredDuringSchedulingIgnoredDuringExecution         Preffered            Ignored                  and node affinity changes will be ignored
requiredDuringSchedulingRequiredDuringExecution         Required             Required             -> Pods will be evicted if not matching affinity rules

--------------------------------------
Taint and Tolerations vs Node Affinity
--------------------------------------

If we have a scenario such as below:
There are 3 nodes - Red, Blue, Green
We want our pods only to run on these 3 nodes.
Moreover, no other pods should be scheduled on our nodes.

Taints and tolerations does not guaratee that the pod will be scheduled on a tainted node.
It can be scheduled on a node with no taint

If we go for Node Affinity, other pods might end up on our Node, if the nodes are shared.

Hence we have to do both Taint & Tolerations as well as Node Affinity

-----------------------
Multiple container pods
-----------------------
Sometimes need 2 services to work together
Created and destroyed together
No volume sharing
containers section is an array

Patterns for multiple pods:
Ambassador -> If for e.g you have separate DB instances in each env. The app service simply refers to the DB as "DB". The logic of switching to the correct DB
              will be handled by another serice/container in the same pod. Proxying request to the right DB.
Adapter -> In case an intermediate processing is required. For e.g Logs coming from multiple apps need to be formatted before sending to the log location
Sidecar -> A service(for e.g, webserver) needs a logging agent

----------------------------------------------------------------------------------------------------------------------------------------------------------------
Observability in Kubernetes
----------------------------------------------------------------------------------------------------------------------------------------------------------------

----------------
Readiness Probes
----------------

Pod has Pod Status and Pod conditions
Pod status tells us where the Pod is in the lifecycle

Pending -> scheduler tries to schedule
Container Creating -> All images are pulled
Running

Conditions complement Pod Status, it's an array of true or false
PodScheduled
Initialized
ContainersReady
Ready

kubectl describe pods command, look for conditions section

"Ready" conditions -> Application is ready and willing to accept user traffic
The pod status may show ready, but how does kubernetes know if the container is really running or not?
By default, as soon as container is created, pod is ready to serve traffic and pod is set to ready state
"Ready" condition should be tied to application ready

Different ways : http, tcp, command

spec:
    containers:
    - name: xx
      image: xx
      readinessProbe:
        httpGet:
            path: /api/ready
            port: 8080
        tcpSocket:
            port: 3306
        exec:
            command:
                - cat
                - /app/is_ready
        initialDelaySeconds: 10 -> We know app will be ready after 10 seconds
        periodSeconds: 5 -> Frequency of probing
        failureThreshold: 8 -> By default, kubernetes will check 3 times for success/failure

---------------
Liveness Probes
---------------
If the application start up but after some time goes in an application error(such as infinite loop)
Kubernetes by default may still route traffic to the app thinking it's still up.
As a developer, you can define when an application is healthy
For a web app, when the api server is up and running etc.

Liveness probe will check periodically if the pod is still working as expected
If not, it may try to restart it or kill it and bring up a new one

    livenessProbe:
        httpGet:
            path: /api/ready
            port: 8080
        tcpSocket:
            port: 3306
        exec:
            command:
                - cat
                - /app/is_ready
        initialDelaySeconds: 10 -> We know app will be ready after 10 seconds
        periodSeconds: 5 -> Frequency of probing
        failureThreshold: 8 -> By default, kubernetes will check 3 times for success/failure

-------
Logging
-------
If a docker conatainer is running in detatchd mode, we cannot see the logs on the console:

docker run -d kodekcloud/event-simulator

We can view logs by -f option

Kubernetes - can view the logs using:
kubectl logs -f event-simulator-pod 

Specific to container running in pod

If there are multiple containers within the pod, you have to specify the container specifically
kubectl logs -f event-simulator-pod event-simulator

------------------
Monitor Kubernetes
------------------
Node and pod level metrics

Metrics Server
1 metrics server per kubernetes cluster. In-memory monitoring solution
Cannot see historical data

kubelet contains cAdvisor -> collects metrics and makes avaialable via api

kubectl top node
kubectl top pod


----------------------------------------------------------------------------------------------------------------------------------------------------------------
POD Design
----------------------------------------------------------------------------------------------------------------------------------------------------------------

----------------------------------
Labels, Selector and Annotatotions
----------------------------------

Labels and selectors are used to group things together

Labels - allow you to group things
Selector - allow you to filter based on criteria

metadata:
    name: simple-webapp
    labels:
        app: App1
        function: Front-end

To filter pods
kubectl get pods --selector app=App1

In replica set, we use selectors to group the pods with matchLabels
The label in matchLabels must match the label on the pod

Annotatotions - 
Used for certain important information
metadata:
    name: simple-webapp
    labels:
        app: App1
        function: Front-end
    annotations:
        buildVersion: 1.34

------------------------------------
Updates and Rollbacks in deployments
------------------------------------
Whenever a new deployment is created, it creates a new rollout, which then creates a new deployment revision
Revision enables us to do Rollbacks

kubectl rollout status deployment/myapp-deployment

kubectl rollout history deployment/myapp-deployment

Two types of deployment strategies:
Recreate Strategy: During rollout, all old objects are destroyed(all at once) and new objects are created. This cause availabilty issues.
Rolling update: During rollout, old ones are deleted and new ones are created one by one. Default strategy

Modify existing deployment definition and then to apply changes:
kubectl apply -f deploy-def.yml
OR
kubectl set image deploymennt/app-deployment image=xxx

kubectl describe deployment

Deployment creates new replica sets and brings up/down pods in old and new

If we want to rollback the deployment:
kubectl rollout undo deployment/myapp-deployment

kubectl run nginx --image=nginx
This actually creates a deployment

----
Jobs
----
Batch processing - meant to be short-running
If you create a pod with a short-running process, the pod completes but then is restarted by Kubernetes again
This is because the restart policy is set to Always by default

spec:
    containers:
    - name: math-add
      image: ubuntu
      command: ['expr','3','+','2']
    restartPolicy: Always

restartPolicy - Always, Never, OnFailure

Never - runs fine for short running

However we sometime require processing on large data sets with multiple pods
We need a manager to create as many pods as we want
ReplicaSet - make sure x no. of pods are running
Jobs - make sure a set of pods are created and run to completion

Similar to ReplicaSet

apiVersion: batch/v1
kind: Job
metadata: 
    name: math-add-Job
spec:
    completions: 3 -> Set no. of pods to 3
    parallelism: 3 -> Creates 3 pods at once
    backoffLimit: 25 # This is so that the job does not quit before it succeeds
    template:
        spec:
            containers:
                - name: math-add
                  image: ubuntu
                  command: ['expr','3','+','2']
            restartPolicy: Never

kubectl create -f xxx.yml
kubectl get jobs -> Shows in a completed
kubectl logs math-add-job-xx
kubectl delete job xxx

By default, a pod is created only after 1st one is finished
What if a pod randomy fails?
Job tries to create a pod until it has "x" successful completions as specified in the "completions" tag

Cron Jobs:
---------
Job which can be scheduled

apiVersion: batch/v1beta1
kind: CronJob
metadata: 
    name: math-add-cron-job
spec:   -> Spec for cron
    schedule: "*/1 * * * *"
    completions: 3 -> Set no. of pods to 3
    parallelism: 3 -> Creates 3 pods at once
    jobTemplate:
       spec:    -> One for job
        completions: 3 -> Set no. of pods to 3
        parallelism: 3 -> Creates 3 pods at once
        template:
            spec:   -> One for pod
                containers:
                    - name: math-add
                    image: ubuntu
                    command: ['expr','3','+','2']
                restartPolicy: Never

kubectl create -f xxx.yml
kubectl get cronjob -> Shows in a completed


--------
Services
--------
Internal and external communication with the user or other pods

For a pod : there is an internal IP address
How do you communicate externally with the pod? 

You can do curl http://10.244.0.2 -> This is the pod internal IP address
But this is only when you login to the node
As a user, i want direct access to the application without having to SSH into the node

Map user request via the node to the pod
Listen to a port on the node -> Forward the request to the app port on the pod

Node port service : Listen to port on the node, map to pod
Cluster IP: Creates a virtual IP within the node
Load Balancer: Provisions a load balancer with an external cloud provider

Node Port:
----------
3 ports involved:
    - Target Port : port on the pod
    - Port: Port on the service itself
        Cluster IP: IP address of the service object
    - Node Port: Accessible by user, range between 30000 - 32767
            -------------------------------------------------------
            |
            |               Cluster IP: 
            |               10.106.1.12
            |               ----------               --------------
          Node Port         |  Port  |               | Target Port|
User------>300008---------> |   80   | --------------|-->   80    |
            |                ---------                -------------
            |               Service Object              Pod
            |
        Kubernetes Node----------------------------------------------

apiVersion: v1
kind: Service
metadata:
    name: myapp-service
spec:
    type: NodePort
    ports:
        - targetPort: 80    -> If not provided, assumed to be the same as port. Actual pod port
          port: 80      -> This is the only mandatory field. The actual service port      
          nodePort: 30008  -> If not provided, any random is assinged within the range
    selector:           -> This actually links the service to the pod. Without it, it's useless
        app: myapp         The values here are the same which are specified on the pod
        type: front-end    

kubectl create -f xxx
kubect get services

If there are multiple pods, the service will route the traffic to all the pods based on the selector
Follows a random algoritm
Can apply seesion affinity
Handles pods on different nodes, same nodes etc.
Balance traffic between different services on different *nodes* in the cluster
    - All services are exposed on the same NodePort
Pods are removed or added -> Service would handle it.

Cluster IP:
-----------
Default for any service
Group pods using service
Useful for Microservices where each layer needs to communicate with other layer
So you can have webservice, redis-service etc.
Is not exposed to the outside world

apiVersion: v1
kind: Service
metadata:
    name: backend
spec:
    type: ClusterIP   
    ports:
        - targetPort: 80  -> If not provided, assumed to be the same as port. Actual pod port
          port: 80      -> This is the only mandatory field. The actual service port      
    selector:           -> This actually links the service to the pod. Without it, it's useless
        app: myapp         The values here are the same which are specified on the pod
        type: front-end    

----------------
Network policies
----------------
Ingress(incoming) and Egress(outgoing) rules

Kubernetes by default has an "All Allow" policy which allows a pod to communicate with any other internal objects

Network policy can only be applied to a pod

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
    name: db-policy
spec:
    podSelector:
        matchLabels:
            role:db -> Apply network policy to the db pod
    policyTypes:
    - Ingress
    ingress:
        - from:
            - podSelector:
                matchLabels:
                    name: api-pod   -> Only allow incoming traffic from the api-pod and no other pods
          ports:
          - protocol: TCP
            port: 3306

The above policy makes sure that the traffic to the db pod is coming in from the api pod. It rejects any other incoming traffic

N/W polcies are imposed by network solutions in kubernetes. Only a few support network policies.
Flannel doesn't support n/w policies.

------------------
Ingress Networking
------------------
Ingress allows you to configure a single URL for a application which you can configure to route internally to multiple services
(As opposed to exposing multiple load balancers for each service and then having a global load balancer)
Also can configure HTTPs 
Think of it as a Layer7 loadbalancer
You still need to expose it to the outside world, hence publish it as a NodePort or as a Load balancer

Consists of two parts:
Ingress Controller - a pre-built solution such as Nginx, HAProxy, Traefik
Ingress Resources - For configuring the controller

Kubernetes cluster by default doesn't come with Ingress Controller
So even if you have resources defined, they won't work

Ingress Controller:
-------------------
Nginx and GCP HTTPs are supported
They are deployed as pods in kubernetes

apiVersion: extensions/v1beta1
kind: Deployment
metadata: 
    name: nginx-ingress-controller
spec:
    replicas: 1
    selector:
        matchLabels:
            name: nginx-ingress
    template:
        metadata:
            labels:
                name: nginx-ingress
        spec:
            containers:
                - name: nginx-ingress-controller
                  image: xxx
            args:
                - /nginx-ingress-controller
        env:                    -> These are required
            - name: POD_NAME
              valueFrom:
                fieldRef:
                    fieldPath: metadata.name
            - name: POD_NAMESPACE
                valueFrom:  
                    fieldRef:
                        fieldPath: metadata.namespace
        ports:
            - name: http
              containerPort: 80
            - name: https:
              containerPort: 443

  You also need to setup:
  a. A nodeport service to expose the ingress controller to the outside world
  b. A ConfigMap to pass configuration to the controller required for things such as http(s), SSL etc.
  c. A serviceaccount so that the ingress controller can access all the internal objects within the cluster


Ingress Resource
----------------
Routing rules

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
    name: ingress-Whatever
spec:
    backend:
        serviceName: wear-service
        servicePort: 80

kubectl create -f xx

kubectl get ingress

kubectl describe ingress ingress-wear-watch

Ingress rule - applies to top level domain name
------------------------------------------------

Below rule routes traffic as below:
www.my-online-store.com
    - /wear  -> Should route to wear service
    - /watch -> Should route to watch service

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
    name: ingress-wear
spec:
    rules:
    - http:
        paths:
            - path: /wear
              backend:
                serviceName: wear-service
                servicePort: 80
            - path: /watch
              backend:
                serviceName: watch-service
                servicePort: 80


For each top level domain name(rule), you can have different paths where the traffic might be routed to.
For e.g:
www.my-online-store.com
    - /wear
    - /watch
www.wear.my-online-store.com
    - /
    - /returns
    - /support

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
    name: ingress-wear
spec:
    rules:
    - host: my-online-store.com
      http:
        paths:
            - backend:
                serviceName: wear-service
                servicePort: 80
            - path: /watch
              backend:
                serviceName: watch-service
                servicePort: 80
     - host: watch.my-online-store.com
       http:
        paths:
            - backend:
                serviceName: watch-service
                servicePort: 80
            - path: /returns
              backend:
                serviceName: returns-service
                servicePort: 80
            - path: /support
              backend:
                serviceName: support-service
                servicePort: 80

If you don't specify any host, it applies to all traffic coming from the ingress

Rewrite target option:
https://kubernetes.github.io/ingress-nginx/examples/rewrite/

When user visits the URL on the left, his request should be forwarded internally to the URL on the right. 
Note that the /watch and /wear URL path are what we configure on the ingress controller so we can forwarded users to the appropriate application in the backend. 
The applications don't have this URL/Path configured on them
http://<ingress-service>:<ingress-port>/watch --> http://<watch-service>:<port>/

Without the rewrite-target option, this is what would happen:
http://<ingress-service>:<ingress-port>/watch --> http://<watch-service>:<port>/watch

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test-ingress
  namespace: critical-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /pay
        backend:
          serviceName: pay-service
          servicePort: 8282


---------------------
Storage in Kubernetes
---------------------

Storage in Docker
2 Types: Storage drivers and volume drivers

Storage drivers
---------------
Help manage storage on images and containers

How docker stores data in File system
/var/lib/docker
    - aufs
    - containers
    - image
    - volumes

Docker layered architecture
Each instruction in dockerfile creates an image with just the changes from the previous layer

If there are 2 docker files with the same instructions except just the app code, docker will reuse all the rest of the layers
so as to save disk space

docker build -> Creates Read-only image layers
docker run -> Creates a writable container layer based on the image layer to store files generated by the container

Same image layer is shared by multiple containers
Life of container layer is ephemeral

COPY-ON-WRITE: if we want to modify a file in the image layer, docker will create a copy in the container layer so that we can edit it
All changes will be done on this local copy

What if we want to persist the container layer
Create a volume

Volume mounting
---------------
docker volume create data_volume
docker run -v data_volume:/var/lib/mysql mysql

docker run -v data_volume2:/var/lib/mysql mysql -> Docker will automatically create volume if it doesn't exist

Bind mounting
-------------
What if we have data at another location?
We can bind volume at any location to the container
docker run -v /data/mysql:/var/lib/mysql mysql

-v is deprecated , --mount-type
docker run --mount-type=bind, source=/data/mysql, target=/var/lib/mysql mysql

Who is responsible for creating layers, mounting volumes etc in docker? 
Storage drivers
Docker will choose driver based on OS -- Device Mapper, Overlay etc.

Volume drivers
--------------
If we want to persist storage, we create volumes - handled by volume drivers

Volume driver plugins - default is local
Convoy, gce-docker, RexRay etc.

docker run -it \ --volume-driver rexray/ebs

Volumes in Kubernetes
---------------------
Docker containers are meant to be transient, and we use volumes to persist data
Similarly, kubernetes pods are ephemeral and we need volumes to persist data

spec:
    containers:
        - image:xx
          xxx
          volumeMounts:
          - mountPath: /opt     -> Mount it on a pod and use as /opt in the pod
            name: data-volume

volumes
- name: data-volume
  hostPath:
    path: /data         -> Use /data directory on node as a volume
    type: Directory

Volume types: 
The above will not work in a multi-node environment
Different providers: CEPH, NFS, AWS, Google Cloud etc.

For e.g for AWS EBS:
volumes
- name: data-volume
  awsElasticBlockStore:
   volumeID: <volume-id>
   fsType: ext4

Persistent Volumes (admin creates)
------------------
In the previous section, users will need to change all pod definitions since it's directly in the pod
We would like to manage storage more centrally

Persistent volumes
Users can claim them using Persistent volume claims

apiVersion:v1
kind: PersistentVolume
metadata:
    name: pv-vol1
spec:
    accessModes:
        - ReadWriteOnce     -> ReadOnlyMany, ReadWriteOnce, ReadWriteMany
    capacity:
        storage: 1Gi
    hostPath:           -> Volume types, can be AWS, Node etc
        path: /tmp/data

kubectl create -f xxxl
kubectl get persistentvolume

Persistent Volume Claims (user creates)
------------------------
Make storage avaialble to a node
A PVC Claim is bound to a single Persistent Volume
If there are multiple matches to a single claim, you can still use labels and selectors to bind the correct PV
If there are no PV's avaialable, PVC will remain in a pending state unless all the criteria's are satisfied and new volumes are avaialable

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
    name: myclaim
spec:
    accessModes:
        - ReadWriteOnce
    resources:
        requests:
            storage:
                500Mi

When above claim is created, kubernetes looks at the access modes and storage requested. Since pv-vol1 is available it's bound(even if total storage is greater)

kubectl create -f xxx
kubectl get persistentvolumeclaim
kubectl delete persistentvolumeclaim myclaim

What happens to volume when claim is deleted? 
By default the volume is set to retain
persistentVolumeReclaimPolicy: Retain or Delete(pvc delete, volume deleted) or Recycle(data in data volume will be scrubbed before making avaialable)

How to use in a POD:
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim

Mount an existing file present on node - node01 - asked in ckad

If PVC is being used in a POD and we delete the PVC directly, it's stuck in a terminating phase
Once POD is deleted the PVC is deleted and the PV is in a released state(in case of default)

Topics not needed for CKAD
--------------------------

Storage classes:
----------------
Used if we want to automatically create volume when app requires it
Define a provisioner: automatically create volume in let's say google cloud
This is called dynamic provisioning
PVC points to Storage class -> No need of manually creating a PV, created automatically by storage class
Many other provisioners -> Azure, Flexer etc.

Stateful Sets:
--------------
Single master, multi slave -> All writes come to master, then duplicated to slave
If there are 2 slaves, 
master -> slave1
slave1 -> slave2

Similar to deployment sets
Pods are created in a sequential order
Ensure master is initialized first
Assign a unique original index to the first pod
Each pod gets a unique name with the statetful set name + index name - you can be sure about the pod names

Definition file - similar to deployment set, except the below:
kind: StatefulSet
Also requires a service name:
serviceName: mysql-h    -> Headless service name

When stateful set is deleted, or scaled down, the last pod is the first one to be brought down. You can set the "podManamgementPolicy" field to change this behavior

Headless Services
-----------------
In case of a cluster(such as MySQL) where you have master and slave nodes, the reads can go to all the pods, but the writes should go to the master only
How do you access only the master pod directly?
One way is by the POD external IP address, but they can't be used since they are dynamic

Headless Service:
We need a service which does not load balance requests but gives a way to reach each pod
Creates DNS service for each pod with the pod name.
So when you create a headless service such as mysql-h, each pod will have a DNS entry such as below:
podname.headless-servicename.namespace.svc.cluster-domain.example
So in case of a master pod,
mysql-0.mysql-h.default.svc.cluster.local -> This should always work

apiVersion: v1
kind: Service
metadata:
    name: mysql-h
spec:
    ports:
        - port: 3306
    selector:
        app: mysql
    ClusterIP: None -> This is what creates a headless service

In the pod definition file, you must specify the below fields to create the automated hostname
spec:
 --
 --
 subdomain: mysql-h   -> Name of the headless service, but doesn't create a-name record
 hostname: mysql-pod  -> Creates a DNS record

If you include the above pod spec in a deployment specification, the pod's created will have the exact same hostname for all pod's
This doesn't solve our purpose in case of a master-slave architecture

This is how a stateful set differs from deployment in that it doesn't require the above 2 fields.
It will automatically generate the correct hostname.

However, how does the stateful set know which headless service it should point to?
In that case, you must specify the serviceName explicitly in the StatefulSet defintion file
spec:
    serviceName: mysql-h

Storage in StatefulSets
-----------------------
In deployments, if you specify the same volume using PVC, it's used by all the pods
Note that not all volume types will support this kind of behavior
What if we want different volumes/PVC's for different pods?(such as in case of a DB):
We can achieve that using a VolumeClaimTemplate. This will automatically create PVC's for each pod using the template.
When a pod is deleted in a stateful set, the PVC is not deleted but then reattached to the same pod once it comes up.